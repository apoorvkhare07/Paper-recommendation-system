{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob \n",
    "import pprint\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import csv\n",
    "import nltk\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "597"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_path = \"20100825-SchPaperRecData/RecCandidatePapersFV/\"\n",
    "files = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "#pprint (glob.glob(\"20100825-SchPaperRecData/RecCandidatePapersFV/*.txt\"))\n",
    "no_of_files=len(files)\n",
    "no_of_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "597"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = []\n",
    "for x in range(no_of_files):\n",
    "    filename=dir_path+files[x]\n",
    "    with open(filename) as f:\n",
    "        reader = csv.reader(f, delimiter=' ')\n",
    "        tmp_data = [(col1, float(col2))\n",
    "                for col1, col2 in reader]\n",
    "    raw_data.append(tmp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' a abl abstract account accumul acl ad add adjac ag aho al algorithm align allow altern amount angbracketleftbig angbracketrightbig annot annual appear approach arbitrari archic art assum at avoid b base beta bilingu binar binari bine box brown calli cfg channel chiang chroniz class classic clue coder combin complex comput confer consist constitu constitut constrain constraint context contigu contribut corpu correspond cost coupl data de decod decompos denomin depend der deriv direct directli discontigu diverg domin drop duce duction e earlei earli easili effici ei eign ek empti en english equat error ex exist exp experi experiment explain explod exploit extract factor fea fere fix fk fluenci follow for foreign form formal frame function gener giza glish glue gnf gram grammar greibach guag hajim hand heurist hideki hier hierarch higher hmei hole hook huang hypothes i icant implement impli improv in incorpor increas indic induc ing initi insert instanc instanti integr intern introduc isozaki item itg j japan japanes juli knowledg koehn kyoto lan languag late lation left lemat length leviat li linguist linguisti link local log long m machin mal man manner match maxim maximum meet mem ment method mhme minim model modern modif move n nal nei newswir ngram non nor normal notat note number numer och of oj on optim or order oriz page pair par parenleftbigsummationtextm parenrightbig parser part pendent perform phrase pliciti possibl posterior pr pre prefer prei present preserv pro prob probabl problem procedur proceed produc product project provid quadrat rank re realiz reduc regard reorder repres requir respec restrict result rewrit right rule run s scenario section seek segment sent sentenc sequenc sequenti serial show side signif signific sim simpl simpli size solut solv sort sourc sov spars spectiv spurious st sta state statist step straightforward strength strict string structur style summationtext svo sydnei symbol syn synchron system take target taro tarotsukadaisozaki task techniqu termi termin th threshold time tion tistic tive top tract tran transform transla translat tree tsukada ture turn type ullman unit violat watanab weight with word work wu'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = []\n",
    "for x in range(no_of_files):\n",
    "    temp_doc=[]\n",
    "    l=len(data[x])\n",
    "    for t in range(l):\n",
    "        temp_doc.append(data[x][t][0])\n",
    "    doc.append(temp_doc)\n",
    "documents=[]\n",
    "print(data[1][2][0])\n",
    "for x in range(no_of_files):\n",
    "    tmp=''\n",
    "    for i in range(len(data[x])):\n",
    "        tmp = tmp+' '+data[x][i][0]\n",
    "    documents.append(tmp)\n",
    "(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    " nltk.download('wordnet') # first-time use only\n",
    " lemmer = nltk.stem.WordNetLemmatizer()\n",
    " def LemTokens(tokens):\n",
    "     return [lemmer.lemmatize(token) for token in tokens]\n",
    " remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    " def LemNormalize(text):\n",
    "     return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37395\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer( norm=\"l2\", stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "print len(tfidf_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(597, 37395)\n"
     ]
    }
   ],
   "source": [
    "print tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.054855308342911566"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "res=cosine_similarity(tfidf_matrix[0:1], tfidf_matrix)\n",
    "(res[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.13811902502207435, 0.13860011321498883, 0.13958153808531823, 0.1402454386187056, 0.14461851888523908, 0.1456784489803089, 0.1535825619340462, 0.15627868862888944, 0.17090570952263884, 1.0000000000000002]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result=res.tolist()\n",
    "result.sort()\n",
    "(result[0].sort())\n",
    "result[0][-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
